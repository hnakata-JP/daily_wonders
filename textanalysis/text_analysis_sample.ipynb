{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643ef69e",
   "metadata": {},
   "source": [
    "Sample notebook for the text analysis using a Slack channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bdbd0b",
   "metadata": {},
   "source": [
    "### initial settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import japanize_matplotlib\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "import slack_sdk\n",
    "import emoji\n",
    "import MeCab\n",
    "_mecab_path = \"-r /dev/null -d /opt/homebrew/lib/mecab/dic/mecab-ipadic-neologd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416aca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from morphogical_analysis_tool import (\n",
    "    print_channel_ids, get_real_name, joint_messages_with_range,\n",
    "    tokenize_with_pos, reduce_tokens, get_emoji_data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338c88a8",
   "metadata": {},
   "source": [
    "## define client and target channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ebbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_he_token = #{your Slack token here}\n",
    "client = slack_sdk.WebClient(token=_he_token)\n",
    "\n",
    "user_name = {}\n",
    "all_members = client.users_list(token=_he_token)['members']\n",
    "for _u in all_members:\n",
    "    if (_u['name'] != 'slackbot') & _u['deleted'] == False:\n",
    "        user_name[_u['id']] = _u['real_name']\n",
    "    else:\n",
    "        pass\n",
    "user_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0802b01",
   "metadata": {},
   "source": [
    "### search channel ID and define the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4dcc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_channel_ids(client, _he_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb19ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_id = # {your channel id here}\n",
    "channel_name = # {your channel name here}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0755bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_start_date = datetime(2023, 1, 1, 0, 0, 0, tzinfo=ZoneInfo(\"UTC\")).timestamp()\n",
    "_stop_date = datetime(2025, 11, 12, 0, 0, 0, tzinfo=ZoneInfo(\"UTC\")).timestamp()\n",
    "\n",
    "request = client.conversations_history(\n",
    "    channel=channel_id, limit=999, oldest=_start_date, latest=_stop_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23635ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2388859d",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bf18b1",
   "metadata": {},
   "source": [
    "## emoji and post frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf2c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = get_emoji_data(request, user_name=user_name)\n",
    "df0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8c14c",
   "metadata": {},
   "source": [
    "### emoji ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the emoji ranking\n",
    "_actions = []\n",
    "for _a in df0['actions']:\n",
    "    _actions.append(_a.split(','))\n",
    "actions = sum(_actions, [])\n",
    "df2 = pd.DataFrame(actions, columns=['actions'])\n",
    "df2 = df2[df2['actions'] != 'no_emoji']\n",
    "df2 = df2[df2['actions'] != '']\n",
    "df2 = df2[df2['actions'] != 'text']\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.set_title(f'Emoji Ranking Top 20 ({channel_name})', fontsize=20)\n",
    "df2['actions'].value_counts().plot.barh(figsize=(10, 8), color='coral', alpha=0.8, ax=ax)\n",
    "ax.set_ylim(-0.4,19.5)\n",
    "ax.yaxis.set_inverted(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f35ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruser = []\n",
    "emoji0 = []\n",
    "for _i in range(df0.shape[0]):\n",
    "    ruser.append(df0['ruser'][_i].split(\",\"))\n",
    "    emoji0.append(df0['reactions'][_i].split(\",\"))\n",
    "rusers = sum(ruser, [])\n",
    "emojis = sum(emoji0, [])\n",
    "print(len(rusers), len(emojis))\n",
    "df1 = pd.DataFrame({\n",
    "    'ruser': rusers,\n",
    "    'emojis': emojis\n",
    "})\n",
    "df1 = df1[df1['ruser'] != 'no_reply']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7541d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3*7, 3*5+1))\n",
    "fig.suptitle(\"emoji reactions in random channel\", fontsize=20)\n",
    "gs = GridSpec(6, 9, hspace=0.8, wspace=0.4)\n",
    "for _i, _u in enumerate(df1['ruser'].unique()):\n",
    "    ax = fig.add_subplot(gs[_i%6, _i//6])\n",
    "    _df = df1[df1['ruser'] == _u]\n",
    "    _df['emojis'].value_counts().plot.barh(\n",
    "        ax=ax, color='lightblue', edgecolor='black')\n",
    "    _keys = [_k[1] for _k in np.array(_df.value_counts().keys())]\n",
    "    ax.set_title(_u)\n",
    "    ax.set_xlim(0,18)\n",
    "    ax.set_xlabel(\"count\")\n",
    "    ax.set_ylim(-0.5,4.5)\n",
    "    ax.set_yticks(range(0, 5),)#labels=_keys[:5])\n",
    "    ax.yaxis.set_inverted(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e2a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "598bc488",
   "metadata": {},
   "source": [
    "### coffee-break entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc2004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mins(x):\n",
    "    x = datetime.fromtimestamp(float(x), tz=ZoneInfo(\"Asia/Tokyo\"))\n",
    "    mins = x.hour*60 + x.minute\n",
    "    return mins\n",
    "mins = []\n",
    "for _ts in df0['ts']:\n",
    "    mins.append(get_mins(_ts))\n",
    "df0['mins'] = mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbce7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _k, _v in df0['user'].value_counts().items():\n",
    "    if _v > 10:\n",
    "        print(f\"{_k}: {_v}\")\n",
    "df0['counts'] = df0['user'].map(df0['user'].value_counts())\n",
    "\n",
    "# sort df by value counts of user\n",
    "df0 = df0.sort_values(by='counts', ascending=False)\n",
    "df0 = df0.reset_index(drop=True)\n",
    "df0 = df0[df0['counts'] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6870cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of ts in 1440 min range for each user label\n",
    "fig = plt.figure(figsize=(16,3))\n",
    "ax = plt.axes()\n",
    "ax.set_title(\"Number of posts per persone in coffee-break channel (>=2 posts)\")\n",
    "\n",
    "for _i, _u in enumerate(df0['user'].unique()):\n",
    "    _=ax.hist(df0['mins'][df0['user'] == _u], histtype='barstacked',\n",
    "            bins=np.linspace(0, 1440, 4*24),\n",
    "            alpha=0.8, label=f\"{_i+1}: {_u}({df0['user'].value_counts()[_u]})\",)\n",
    "\n",
    "ax.set_xlabel(\"時間\")\n",
    "ax.set_xlim(0, 1440)\n",
    "ax.set_xticks(np.arange(0, 1440, 60), [f\"{i//60:02d}時\" for i in range(0, 1440, 60)])\n",
    "\n",
    "ax.legend(loc='upper left', ncol=3, fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a066eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abc8165b",
   "metadata": {},
   "source": [
    "### something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c1b2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d824e52",
   "metadata": {},
   "source": [
    "# Morphological Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c26d5c",
   "metadata": {},
   "source": [
    "### define stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1f4614",
   "metadata": {},
   "outputs": [],
   "source": [
    "uninteresting_tokens = [\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j',\n",
    "    'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u',\n",
    "    'v', 'w', 'x', 'y', 'z',\n",
    "    'I', 'you', 'You', 'we', 'We', 'he', 'He', 'she', 'She', 'they', 'They',\n",
    "    'me', 'my', 'mine', 'you', 'your', 'yours',\n",
    "    'his', 'her', 'hers', 'its', 'our', 'ours', 'their', 'theirs', 'it', 'It',\n",
    "    'am', 'are', 'is', 'was', 'were', 'be', 'been', 'being', 'them',\n",
    "    'this', 'This', 'that', 'That', 'These', 'those',\n",
    "    'what', 'which', 'who', 'whom', 'whose',\n",
    "    'at', 'by', 'for', 'from', 'in', 'into', 'of', 'on', 'to', 'with',\n",
    "    'an', 'the', 'The', 'and', 'has', 'will',\n",
    "    'as', 'As', 'if', 'but', 'some',\n",
    "    'like', 'need', 'been', 'have',\n",
    "    'any', 'now', 'not', 'or', 'here', 'one',\n",
    "    'all', 'someone', 'take', 'just', 'other', 'know',\n",
    "    'about', 'able', 'next', 'can', 'ask', 'if',\n",
    "    'If', 'only', 'made', 'write', 'would', 'below',\n",
    "    'https', 'www', '.com', 'ac.jp', 'u.', 'please',\n",
    "    'ｗｉ', 'ｆｉ',\n",
    "    'ア', 'イ', 'ウ', 'エ', 'オ',\n",
    "    'こと', 'よう', 'ため', 'ない', 'いる', 'する', 'ある', 'の',\n",
    "    'これ', 'それ', 'あれ', 'どれ', 'もの', 'そう', 'ところ',\n",
    "    '月日', '号', '号室', '*', '%',\n",
    "]\n",
    "\n",
    "uninteresting_numbers = [str(i) for i in range(100)] +['0'+str(i) for i in range(10)] + ['-'+str(i) for i in range(1, 10)] + [str(i)+'%' for i in range(1, 100)] + ['１', '２', '３', '４', '５', '６', '７', '８', '９', '０']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e92674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwordsの指定\n",
    "with open(\"./English_stopwords.txt\",\"r\") as f:\n",
    "    stopwords0 = f.read().split(\"\\n\")\n",
    "\n",
    "with open(\"./Japanese_stopwords.txt\",\"r\") as f:\n",
    "    stopwords1 = f.read().split(\"\\n\")\n",
    "\n",
    "stopwords = list(set(\n",
    "    stopwords0 + stopwords1 + uninteresting_tokens + uninteresting_numbers\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cedbc9",
   "metadata": {},
   "source": [
    "### get full text and remove urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3515658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "url_pattern = r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+'\n",
    "full_text = joint_messages_with_range(request)\n",
    "full_text = re.sub(url_pattern, '', full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f77a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_with_pos = tokenize_with_pos(full_text, _mecab_path)\n",
    "# df0 = pd.DataFrame(tokens_with_pos, columns=['token', 'pos', 'pos2'])\n",
    "# df0 = df0[df0['pos2'] == '固有名詞']\n",
    "# #df0 = df0[df0['pos'] == '名詞']\n",
    "# df0 = df0[~df0['token'].isin(uninteresting_tokens)]\n",
    "# #df = df[~np.in1d(df['token'], uninteresting_tokens)]\n",
    "# df0 = df0['token'].value_counts().reset_index()\n",
    "# df0.columns = ['token', 'count']\n",
    "# df0 = df0[df0['count'] > 1]\n",
    "# df0 = df0.sort_values('count', ascending=False)\n",
    "# df0 = df0.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ee366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def mecab_tokenizer(text):\n",
    "\n",
    "    replaced_text = text.lower()\n",
    "    replaced_text = re.sub(r'[【】]', ' ', replaced_text)       # 【】の除去\n",
    "    replaced_text = re.sub(r'[（）()]', ' ', replaced_text)     # （）の除去\n",
    "    replaced_text = re.sub(r'[［］\\[\\]]', ' ', replaced_text)   # ［］の除去\n",
    "    replaced_text = re.sub(r'[@＠]\\w+', '', replaced_text)  # メンションの除去\n",
    "    # replaced_text = re.sub(r'\\d+\\.*\\d*', '', replaced_text) #数字を0にする\n",
    "\n",
    "    # path = \"-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
    "    # mecab = MeCab.Tagger(path)\n",
    "    mecab = MeCab.Tagger(_mecab_path)\n",
    "    parsed_lines = mecab.parse(replaced_text).split(\"\\n\")[:-2]\n",
    "\n",
    "    # #表層形を取得\n",
    "    # surfaces = [l.split('\\t')[0] for l in parsed_lines]\n",
    "    #原形を取得\n",
    "    # token_list = [l.split(\"\\t\")[1].split(\",\")[6] for l in parsed_lines]\n",
    "    token_list = [re.split('[\\t,]', l)[0] for l in parsed_lines]\n",
    "    #品詞を取得\n",
    "    pos = [re.split('[\\t,]', l)[1] for l in parsed_lines]\n",
    "    # 名詞のみに絞り込み\n",
    "    target_pos = [\"名詞\"]\n",
    "    token_list = [t for t, p in zip(token_list, pos) if p in target_pos]\n",
    "\n",
    "    # stopwordsの除去\n",
    "    # stopwords = set(uninteresting_tokens)\n",
    "    token_list = [t for t in token_list if t  not in stopwords]\n",
    "\n",
    "    # ひらがなのみの単語を除く\n",
    "    kana_re = re.compile(\"^[ぁ-ゖ]+$\")\n",
    "    token_list = [t for t in token_list if not kana_re.match(t)]\n",
    "\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [mecab_tokenizer(sentence) for sentence in full_text.split(\"。\")]\n",
    "sentences[0], len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20ed2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "sentences = [mecab_tokenizer(sentence) for sentence in full_text.split(\"。\")]\n",
    "sentences_combs = [list(itertools.combinations(sentence,2)) for sentence in sentences]\n",
    "words_combs = [[tuple(sorted(words)) for words in sentence] for sentence in sentences_combs]\n",
    "target_combs = []\n",
    "for words_comb in words_combs:\n",
    "    target_combs.extend(words_comb)\n",
    "\n",
    "import collections\n",
    "ct = collections.Counter(target_combs)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([{\"1番目\" : i[0][0], \"2番目\": i[0][1], \"count\":i[1]} for i in ct.most_common()])\n",
    "print(df.shape)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c903d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kyoki_word_network(df):\n",
    "    from pyvis.network import Network\n",
    "    import pandas as pd\n",
    "\n",
    "\n",
    "    got_net = Network(height=\"1000px\", width=\"95%\", bgcolor=\"#FFFFFF\", font_color=\"black\", notebook=True)\n",
    "\n",
    "    got_net.force_atlas_2based()\n",
    "    # got_data = df[:150]\n",
    "    got_data = df[df['count'] >= 3]\n",
    "\n",
    "    sources = got_data['1番目']#count\n",
    "    targets = got_data['2番目']#first\n",
    "    weights = got_data['count']#second\n",
    "\n",
    "    edge_data = zip(sources, targets, weights)\n",
    "\n",
    "    for e in edge_data:\n",
    "        src = e[0]\n",
    "        dst = e[1]\n",
    "        w = e[2]\n",
    "\n",
    "        got_net.add_node(src, src, title=src)\n",
    "        got_net.add_node(dst, dst, title=dst)\n",
    "        got_net.add_edge(src, dst, value=w)\n",
    "\n",
    "    neighbor_map = got_net.get_adj_list()\n",
    "\n",
    "    for node in got_net.nodes:\n",
    "        node[\"title\"] += \" Neighbors:<br>\" + \"<br>\".join(neighbor_map[node[\"id\"]])\n",
    "        node[\"value\"] = len(neighbor_map[node[\"id\"]])\n",
    "\n",
    "    got_net.show_buttons(filter_=['nodes', 'edges', 'physics'])\n",
    "    return got_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8085e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "got_net = kyoki_word_network(df)\n",
    "got_net.show(\"kyoki_random.html\")\n",
    "from IPython.display import HTML\n",
    "# HTML(\"/content/kyoki.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d61d569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b6ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uninterested2 = [\n",
    "    'zoom', 'Zoom', 'photo', 'photos', 'pdf', 'jp', 'join', 'today', 'tomorrow', 'floor', 'PC',\n",
    "    'Please', 'web', 'Web', 'ing', '.jp', 'post', 'link', 'plan', 'room', 'wifi',\n",
    "    'ht', 'ml',\n",
    "    '-1','-2', '-3', '-4', '-5', '-6', '-7', '-8', '-9', '-10',\n",
    "    'よろしくお願いします', 'ありません',\n",
    "]\n",
    "\n",
    "stopwords1 = list(set(stopwords0 + stopwords1 + uninteresting_tokens + uninterested2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eb1982",
   "metadata": {},
   "outputs": [],
   "source": [
    "_test = mecab_tokenizer(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _list = df0['token'].tolist()\n",
    "_list = _test\n",
    "words_wakachi = [word for word in _list if word not in stopwords1]\n",
    "print(words_wakachi), print(len(words_wakachi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d9e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "\n",
    "# 候補（入っていそうな順）\n",
    "candidates = [\n",
    "    \"IPAexGothic\",        # Homebrew等で入れやすい\n",
    "    \"Hiragino Sans\",      # macOS標準（ヒラギノ角ゴ）\n",
    "    \"Hiragino Kaku Gothic ProN\",\n",
    "    \"Yu Gothic\",\n",
    "    \"Noto Sans CJK JP\",\n",
    "    \"Source Han Sans JP\"\n",
    "]\n",
    "\n",
    "font_path = None\n",
    "for name in candidates:\n",
    "    try:\n",
    "        path = font_manager.findfont(name, fallback_to_default=False)\n",
    "        if os.path.exists(path):\n",
    "            font_path = path\n",
    "            break\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if font_path is None:\n",
    "    raise FileNotFoundError(\"日本語フォントが見つかりません。IPAexGothic などをインストールしてください。\")\n",
    "\n",
    "print(\"Using font:\", font_path)\n",
    "\n",
    "my_stopwords = set(uninteresting_tokens)\n",
    "\n",
    "word_cloud_general = WordCloud(\n",
    "    font_path=font_path,\n",
    "    width=1600, height=900,\n",
    "    stopwords=my_stopwords,\n",
    "    prefer_horizontal=1,\n",
    "    min_font_size=5,\n",
    "    collocations=False,\n",
    "    background_color=\"white\",\n",
    "    max_words=4000\n",
    ").generate(words_wakachi if isinstance(words_wakachi, str) else \" \".join(words_wakachi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(16,9))\n",
    "plt.imshow(word_cloud_general)\n",
    "plt.tick_params(labelbottom=False, labelleft=False)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fdb27a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(18,3))\n",
    "# plt.bar(df0['token'][:100], df0['count'][:100])\n",
    "# plt.title('Top 100 proper nouns in random channel')\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.xlabel('token')\n",
    "# plt.ylabel('count')\n",
    "# plt.grid(lw=0.2)\n",
    "# plt.ylim(0, 30)\n",
    "# plt.xlim(-0.6, 99.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b487aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get monthly messages and token\n",
    "# _base_ts = datetime(2022, 4, 1, 0, 0, 0, tzinfo=ZoneInfo(\"Asia/Tokyo\"))\n",
    "\n",
    "# df = pd.DataFrame(columns=[\"start\", \"token\", \"count\"])\n",
    "# for _i in range(36):\n",
    "#     start = _base_ts + relativedelta(months=_i)\n",
    "#     stop = _base_ts + relativedelta(months=(_i+1))\n",
    "#     #print(f\"{start.strftime('%Y/%m/%d')} - {stop.strftime('%Y/%m/%d')}\")\n",
    "#     messages = get_messages_with_range(request, start, stop)\n",
    "#     tokens_with_pos = tokenize_with_pos(messages)\n",
    "#     _df = pd.DataFrame(tokens_with_pos, columns=[\"token\", \"pos\", \"subpos\"])\n",
    "#     _df = _df[_df[\"subpos\"] == \"固有名詞\"]\n",
    "#     # _df = _df[_df[\"pos\"] == \"名詞\"]\n",
    "#     _df = _df[~_df[\"token\"].isin(unintersted_tokens)]\n",
    "#     _df = _df['token'].value_counts().reset_index()\n",
    "#     # if _i == 0:\n",
    "#     #     print(_df)\n",
    "#     _df = _df[_df['count'] >= 2]\n",
    "#     _df.columns = [\"token\", \"count\"]\n",
    "#     _df[\"start\"] = start\n",
    "#     _df = _df.sort_values(by=\"count\", ascending=False)\n",
    "#     _df = _df.reset_index(drop=True)\n",
    "#     df = pd.concat([df, _df], ignore_index=True)\n",
    "# df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(36,6))\n",
    "# gs = GridSpec(1, 12, figure=fig)\n",
    "# for _i in range(12):\n",
    "#     ax = fig.add_subplot(gs[0, _i])\n",
    "#     _target_ts = _base_ts + relativedelta(months=_i)\n",
    "#     ax.set_title(f\"{_target_ts.strftime('%Y年%m月')}\")\n",
    "#     try:\n",
    "#         _df = df[df[\"start\"] == _target_ts]\n",
    "#         _df.plot(x=\"token\", y=\"count\", kind=\"barh\", ax=ax, legend=False)\n",
    "#         ax.set_ylabel(None)\n",
    "#         ax.set_xlim(0,18)\n",
    "#         ax.set_ylim(-0.4,20.5)\n",
    "#         ax.yaxis.set_inverted(True)\n",
    "#         # remove the frame of the plots\n",
    "#         ax.spines['top'].set_visible(False)\n",
    "#         ax.spines['bottom'].set_visible(False)\n",
    "#         ax.spines['left'].set_visible(False)\n",
    "#         ax.spines['right'].set_visible(False)\n",
    "#         ax.tick_params(labelbottom=False)\n",
    "#     except:\n",
    "#         ax.spines['top'].set_visible(False)\n",
    "#         ax.spines['bottom'].set_visible(False)\n",
    "#         ax.spines['left'].set_visible(False)\n",
    "#         ax.spines['right'].set_visible(False)\n",
    "#         ax.tick_params(labelleft=False, labelbottom=False)\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194882cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slack",
   "language": "python",
   "name": "slack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
